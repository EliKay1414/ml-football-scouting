{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "171be5fb-6f05-463e-a94b-e89a381b06cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-19 03:14:43,544 - INFO -  Successfully imported FootballFeaturePreprocessor\n",
      "2025-08-19 03:14:43,917 - INFO - \n",
      " Loading datasets...\n",
      "2025-08-19 03:14:44,271 - INFO -  Training data loaded: 27377 records\n",
      "2025-08-19 03:14:44,272 - INFO -  Test data loaded: 9126 records\n",
      "2025-08-19 03:14:44,273 - INFO - \n",
      " Initializing FootballFeaturePreprocessor...\n",
      "2025-08-19 03:14:44,307 - INFO - \n",
      "ðŸ”§ Fitting preprocessor on training data...\n",
      "2025-08-19 03:14:44,617 - INFO - Feature quality analysis completed\n",
      "C:\\Users\\Nanaba\\anaconda3\\envs\\final_year_project\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:306: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Nanaba\\anaconda3\\envs\\final_year_project\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:306: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 1 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Nanaba\\anaconda3\\envs\\final_year_project\\lib\\site-packages\\sklearn\\preprocessing\\_discretization.py:306: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 2 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "2025-08-19 03:14:49,247 - INFO - Preprocessor fitted with 180 features\n",
      "2025-08-19 03:14:49,253 - INFO -  Feature quality report saved to C:\\Users\\Nanaba\\Desktop\\football_player_scouting_ml\\data\\processed\\v20250819\\feature_quality_report.csv\n",
      "2025-08-19 03:14:49,254 - INFO - \n",
      " Processing data in chunks of 2000...\n",
      "2025-08-19 03:14:49,499 - INFO - Processed 2000/27377 training records\n",
      "2025-08-19 03:14:49,747 - INFO - Processed 4000/27377 training records\n",
      "2025-08-19 03:14:49,974 - INFO - Processed 6000/27377 training records\n",
      "2025-08-19 03:14:50,217 - INFO - Processed 8000/27377 training records\n",
      "2025-08-19 03:14:50,518 - INFO - Processed 10000/27377 training records\n",
      "2025-08-19 03:14:50,813 - INFO - Processed 12000/27377 training records\n",
      "2025-08-19 03:14:51,082 - INFO - Processed 14000/27377 training records\n",
      "2025-08-19 03:14:51,316 - INFO - Processed 16000/27377 training records\n",
      "2025-08-19 03:14:51,535 - INFO - Processed 18000/27377 training records\n",
      "2025-08-19 03:14:51,760 - INFO - Processed 20000/27377 training records\n",
      "2025-08-19 03:14:52,054 - INFO - Processed 22000/27377 training records\n",
      "2025-08-19 03:14:52,289 - INFO - Processed 24000/27377 training records\n",
      "2025-08-19 03:14:52,639 - INFO - Processed 26000/27377 training records\n",
      "2025-08-19 03:14:52,933 - INFO - Processed 27377/27377 training records\n",
      "C:\\Users\\Nanaba\\AppData\\Local\\Temp\\ipykernel_12348\\2140383907.py:120: DeprecationWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if pd.api.types.is_sparse(train_processed[col]):\n",
      "C:\\Users\\Nanaba\\AppData\\Local\\Temp\\ipykernel_12348\\2140383907.py:124: DeprecationWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if pd.api.types.is_sparse(test_processed[col]):\n",
      "2025-08-19 03:14:53,896 - INFO - \n",
      " Saving processed datasets...\n",
      "2025-08-19 03:14:54,019 - INFO -  Training data saved to C:\\Users\\Nanaba\\Desktop\\football_player_scouting_ml\\data\\processed\\v20250819\\fifa_players_train_processed.feather\n",
      "2025-08-19 03:14:54,020 - INFO -  Test data saved to C:\\Users\\Nanaba\\Desktop\\football_player_scouting_ml\\data\\processed\\v20250819\\fifa_players_test_processed.feather\n",
      "2025-08-19 03:14:54,021 - INFO - \n",
      " Preprocessing complete!\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Set up paths and imports\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy import sparse\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure basic logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Get the absolute path to the src directory\n",
    "src_path = os.path.abspath(r\"C:\\Users\\Nanaba\\Desktop\\football_player_scouting_ml\\src\")\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)\n",
    "\n",
    "# Import with enhanced error handling\n",
    "try:\n",
    "    from feature_scaling import FootballFeaturePreprocessor\n",
    "    logger.info(\" Successfully imported FootballFeaturePreprocessor\")\n",
    "except ImportError as e:\n",
    "    logger.error(f\" Import Error: {e}\")\n",
    "    logger.error(\"\\nContents of src directory:\")\n",
    "    logger.error(os.listdir(src_path))\n",
    "    raise\n",
    "\n",
    "# Step 2: Define file paths with versioning\n",
    "current_date = datetime.now().strftime(\"%Y%m%d\")\n",
    "data_dir = Path(r\"C:\\Users\\Nanaba\\Desktop\\football_player_scouting_ml\\data\\processed\")\n",
    "split_path = data_dir / \"processed\" \n",
    "processed_path = data_dir  / f\"v{current_date}\"\n",
    "processed_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# File paths for input data\n",
    "train_file = data_dir / \"fifa_players_train.csv\"\n",
    "test_file = data_dir / \"fifa_players_test.csv\"\n",
    "train_df = pd.read_csv(train_file, low_memory=False)\n",
    "test_df = pd.read_csv(test_file, low_memory=False)\n",
    "\n",
    "# File paths for processed output\n",
    "train_processed_file = processed_path / \"fifa_players_train_processed.feather\"\n",
    "test_processed_file = processed_path / \"fifa_players_test_processed.feather\"\n",
    "feature_report_file = processed_path / \"feature_quality_report.csv\"\n",
    "\n",
    "\n",
    "# Step 3: Load datasets with memory optimization\n",
    "logger.info(\"\\n Loading datasets...\")\n",
    "try:\n",
    "    train_df = pd.read_csv(train_file, low_memory=False)\n",
    "    test_df = pd.read_csv(test_file, low_memory=False)\n",
    "    logger.info(f\" Training data loaded: {len(train_df)} records\")\n",
    "    logger.info(f\" Test data loaded: {len(test_df)} records\")\n",
    "except Exception as e:\n",
    "    logger.error(\"Data loading failed\", exc_info=True)\n",
    "    raise\n",
    "\n",
    "# Step 4: Initialize preprocessor with enhanced configuration\n",
    "logger.info(\"\\n Initializing FootballFeaturePreprocessor...\")\n",
    "preprocessor = FootballFeaturePreprocessor(\n",
    "    target='is_prospect',  # Updated target focus\n",
    "    model_type='auto',     # Auto-detect best approach\n",
    "    max_categories=75,     # Increased from original 50\n",
    "    n_components=None      # Optional PCA reduction\n",
    ")\n",
    "\n",
    "# Step 5: Create prospect target if needed\n",
    "if 'is_prospect' not in train_df.columns:\n",
    "    logger.info(\"Creating prospect target variable...\")\n",
    "    train_df['is_prospect'] = preprocessor.create_prospect_target(train_df)\n",
    "    val_df['is_prospect'] = preprocessor.create_prospect_target(val_df)\n",
    "    test_df['is_prospect'] = preprocessor.create_prospect_target(test_df)\n",
    "    logger.info(f\"Prospect counts - Train: {train_df['is_prospect'].sum()}, Val: {val_df['is_prospect'].sum()}, Test: {test_df['is_prospect'].sum()}\")\n",
    "\n",
    "# Identify target columns\n",
    "target_cols = ['overall', 'potential', 'is_prospect']\n",
    "feature_cols = [col for col in train_df.columns if col not in target_cols]\n",
    "\n",
    "# Separate features and targets\n",
    "X_train = train_df[feature_cols].copy()\n",
    "y_train = train_df[target_cols].copy()\n",
    "\n",
    "\n",
    "X_test = test_df[feature_cols].copy()\n",
    "y_test = test_df[target_cols].copy()\n",
    "\n",
    "# Step 6: Fit and transform data\n",
    "logger.info(\"\\nðŸ”§ Fitting preprocessor on training data...\")\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "# Save feature quality report\n",
    "feature_report = preprocessor.get_feature_quality_report()\n",
    "feature_report.to_csv(feature_report_file)\n",
    "logger.info(f\" Feature quality report saved to {feature_report_file}\")\n",
    "\n",
    "# Process data in chunks (memory-efficient)\n",
    "chunk_size = 2000\n",
    "logger.info(f\"\\n Processing data in chunks of {chunk_size}...\")\n",
    "\n",
    "def process_in_chunks(X, name):\n",
    "    chunks = []\n",
    "    for i in range(0, len(X), chunk_size):\n",
    "        chunk = X.iloc[i:i+chunk_size]\n",
    "        processed_chunk = preprocessor.transform(chunk)\n",
    "        chunks.append(processed_chunk)\n",
    "        logger.info(f\"Processed {min(i+chunk_size, len(X))}/{len(X)} {name} records\")\n",
    "    return pd.concat(chunks)\n",
    "\n",
    "X_train_processed = process_in_chunks(X_train, 'training')\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "# Combine processed features with targets\n",
    "train_processed = pd.concat([X_train_processed, y_train.reset_index(drop=True)], axis=1)\n",
    "test_processed = pd.concat([X_test_processed, y_test.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Convert sparse columns to dense (fix for Feather saving)\n",
    "for col in train_processed.columns:\n",
    "    if pd.api.types.is_sparse(train_processed[col]):\n",
    "        train_processed[col] = train_processed[col].sparse.to_dense()\n",
    "\n",
    "for col in test_processed.columns:\n",
    "    if pd.api.types.is_sparse(test_processed[col]):\n",
    "        test_processed[col] = test_processed[col].sparse.to_dense()\n",
    "\n",
    "# Step 7: Save processed datasets\n",
    "logger.info(\"\\n Saving processed datasets...\")\n",
    "train_processed.to_feather(train_processed_file)\n",
    "test_processed.to_feather(test_processed_file)\n",
    "logger.info(f\" Training data saved to {train_processed_file}\")\n",
    "logger.info(f\" Test data saved to {test_processed_file}\")\n",
    "\n",
    "logger.info(\"\\n Preprocessing complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
